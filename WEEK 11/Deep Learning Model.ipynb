{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU Device Name: NVIDIA GeForce RTX 2070 SUPER\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Memastikan penggunaan GPU jika tersedia\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Device Name:\", torch.cuda.get_device_name(0))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\B'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\B'\n",
      "C:\\Users\\Mizan\\AppData\\Local\\Temp\\ipykernel_12316\\241991062.py:3: SyntaxWarning: invalid escape sequence '\\B'\n",
      "  data = pd.read_csv('E:\\Bot\\MLD\\heart.csv')\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Dataset\n",
    "# Penjelasan: Membaca dataset dari file CSV.\n",
    "data = pd.read_csv('E:\\Bot\\MLD\\heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocessing\n",
    "# Penjelasan: Memisahkan fitur (X) dan target (y), serta melakukan normalisasi dataset.\n",
    "X = data.drop(columns=['target']).values\n",
    "y = data['target'].values\n",
    "\n",
    "# Normalisasi fitur menggunakan StandardScaler.\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Membagi data menjadi training dan testing set (80% train, 20% test).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Mengubah data ke dalam bentuk Tensor.\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# Membuat DataLoader untuk pelatihan dan pengujian.\n",
    "batch_sizes = [16, 32, 64, 128, 256, 512]\n",
    "train_datasets = [DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True) for batch_size in batch_sizes]\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Definisi Model\n",
    "# Penjelasan: Membuat arsitektur MLP dengan jumlah layer dan neuron yang dapat dikonfigurasi.\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, neurons_per_layer, activation_fn):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "\n",
    "        # Input Layer\n",
    "        layers.append(nn.Linear(input_size, neurons_per_layer))\n",
    "        layers.append(activation_fn)\n",
    "\n",
    "        # Hidden Layers\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.append(nn.Linear(neurons_per_layer, neurons_per_layer))\n",
    "            layers.append(activation_fn)\n",
    "\n",
    "        # Output Layer (2 kelas: 0 dan 1)\n",
    "        layers.append(nn.Linear(neurons_per_layer, 2))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eksperimen selesai. Hasil disimpan di 'experiment_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "# 4. Pengaturan Eksperimen\n",
    "hidden_layers_options = [1, 2, 3]\n",
    "neurons_options = [4, 8, 16, 32, 64]\n",
    "activation_functions = {\n",
    "    'linear': nn.Identity(),\n",
    "    'sigmoid': nn.Sigmoid(),\n",
    "    'relu': nn.ReLU(),\n",
    "    'softmax': nn.Softmax(dim=1),\n",
    "    'tanh': nn.Tanh()\n",
    "}\n",
    "epochs_options = [1, 10, 25, 50, 100, 250]\n",
    "learning_rates = [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "# Fungsi pelatihan model\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs):\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    accuracy_history = []\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # Loop setiap batch pada data training\n",
    "        for batch in train_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward pass dan optimasi\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Evaluasi pada data testing\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "\n",
    "        train_loss_history.append(train_loss / len(train_loader))\n",
    "        test_loss_history.append(test_loss / len(test_loader))\n",
    "        accuracy_history.append(correct / total)\n",
    "\n",
    "    return train_loss_history, test_loss_history, accuracy_history\n",
    "\n",
    "# Loop Eksperimen\n",
    "results = []\n",
    "\n",
    "for hidden_layers in hidden_layers_options:\n",
    "    for neurons in neurons_options:\n",
    "        for activation_name, activation_fn in activation_functions.items():\n",
    "            for epochs in epochs_options:\n",
    "                for lr in learning_rates:\n",
    "                    for train_loader in train_datasets:\n",
    "                        model = MLP(X_train.shape[1], hidden_layers, neurons, activation_fn)\n",
    "                        criterion = nn.CrossEntropyLoss()\n",
    "                        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                        # Latih model\n",
    "                        train_loss, test_loss, accuracy = train_model(model, train_loader, test_loader, criterion, optimizer, epochs)\n",
    "\n",
    "                        # Simpan hasil\n",
    "                        results.append({\n",
    "                            'hidden_layers': hidden_layers,\n",
    "                            'neurons': neurons,\n",
    "                            'activation': activation_name,\n",
    "                            'epochs': epochs,\n",
    "                            'learning_rate': lr,\n",
    "                            'batch_size': train_loader.batch_size,\n",
    "                            'accuracy': accuracy[-1]\n",
    "                        })\n",
    "\n",
    "# Konversi hasil eksperimen ke DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('experiment_results.csv', index=False)\n",
    "\n",
    "print(\"Eksperimen selesai. Hasil disimpan di 'experiment_results.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
